<h2 style="text-align:center">《并行计算》上机报告</h2>

<table>
	<tr>
		<td>姓名：</td>
		<td>孙玉鑫</td>
		<td>学号：</td>
		<td>PB16020865</td>
		<td>日期：</td>
		<td>2019/5/25</td>
	<tr/>
	<tr>
		<td>上机题目：</td>
		<td colspan="5" style="text-align:center">MPI编程</td>
	</tr>
	<tr>
		<td rowspan="2">实验环境</td> 
		<td  colspan="2" >CPU</td>
		<td>内存</td>
		<td>操作系统</td>
		<td>软件平台</td>
	</tr>
	<tr>
		<td colspan="2">Intel i7-6700HQ ×8</td>
		<td>8G</td>
		<td>Arch Linux</td>
		<td>不清楚</td>
	</tr>
</table>
<h4 style="text-align:center">PI值计算</h4>


### 一、算法设计与分析

#### 题目：用MPI计算$\pi$值

#### 算法设计：

​		本次实验利用积分$\,4\int_0^1\frac{1}{1+x^2}\mathrm{d}x=4 \arctan(1) = \pi\,$来计算$\pi$值。计算时，根据指定的$\mathrm{step}$值，将$\left[0,1\right]$区间循环分配给每个处理器。每个处理器在当前区间内利用复化Simpson积分，算出该区间对应的值，并与先前结果进行累加。每个处理器在自己分配的区间完成后，将累加和发送给$0$号处理器。

### 二、结果与分析：

> mpiicc pi.c -O -o pi.out && mpirun -n 4 ./pi.out
>
> --> PI = 3.14159265358979
>
>  total steps:   20000
>
>  total cores:   4

​		结果非常接近$\pi$值，采用了精度为步长的二阶小量的Simpson算法，实际实验总步数为$100$左右时，已经具有至少$14$位小数的精度。

#### 三、备注：

​		本实验较为简单，不包含复杂操作，因此略去核心代码等内容。

----




<h4 style="text-align:center">PSRS排序</h4>

### 一、算法设计与分析

#### 题目： 用MPI实现PSRS排序



#### 算法设计：

   	 给定处理器数目$\mathrm{N},\,\mathrm{id}=0,\,1,\,2,\cdots,\mathrm{N}-1$，$0$号处理器将长度为$\mathrm{L}$的数组分配给每个处理器,经过PSRS排序后返回给$0$号处理器。

+ 分段：$0$号线程把数组划分为$\mathrm{N}$段，每段长度为$\mathrm{floor}(\frac{L}{N})$或者$\mathrm{floor}(\frac{L}{N})+1$，保证每个线程得到的数组长度相差$1$以内。
+ 数据分发：$0$号线程利用`MPI_Scatter()`函数，把每段数据的长度告诉其他线程，其他线程收到后据此申请内存空间；接着利用`MPI_Scatterv()`函数，把数据发送给其他线程。

+ 快速排序：每个线程调用`quick_sort()`（见头文件）对自己负责的数据进行原地排序。
+ 正则采样：每个线程计算$\mathrm{step2}=\mathrm{floor}(\frac{\mathrm{step}}{\mathrm{N}})$，并得到所有组内下标是 $\mathrm{step2}$ 倍数的数据，然后根据自己$\mathrm{id}$写入正则采样数组 $\mathrm{reg}[N]$ 的相应位置。
+ 主元收集：利用`MPI_Gather()`函数,将$\mathrm{N}$个线程中的主元都收集到$0$号线程的$\mathrm{regAll}$中。
+ 选择主元：$0$号线程对数组$\mathrm{regAll}$中的数据排序，然后以$N$为间隔，选出中间的$N-1$个数据，放在数组$\mathrm{reg}$的前$N-1$个位置上。
+ 广播主元：利用`MPI_Bcast()`，把主元广播给所有线程。
+ 主元划分：每个线程根据$\mathrm{N}-1$个主元，将自己的数据分为$\mathrm{N}$段，准备接下来的全局交换。

+ 全局交换：每个线程算出划分后的每段数据长度，利用`MPI_Alltotal()`告诉其他线程，完成数据长度信息的全局交换。之后每个线程根据收到的信息申请内存空间，调用`MPI_Alltotalv()`，对数据进行全局交换。
+ 归并排序：每个线程调用`merge_sort()`（见头文件）。
+ 收集：调用`MPI_Gather()`与`MPI_Gatherv()`，把每个线程的数据发给主线程。



#### 算法分析：

​    	每个线程进行快速排序与归并排序的时间平均为$O\big(\frac{L}{N}\log\frac{L}{N}\big)$， 采样、选择主元为$O(N)$，对主元的排序为$O(N^2\!\log{N^2)}$，主元划分操作为$O\big(\frac{L}{N}\big)$，全局交换等通讯时间为$O(L)$。

​		算法的总时间为 $t(L)=O\big(\frac{L}{N}\log\frac{L}{N}+N^2\log{N^2}\big)=O\big(\frac{L}{N}\log L\big)$。



### 二、核心代码：

方便起见，定义一个由裸数组指针与长度组成的结构体，方便记录数据信息。

```c
typedef struct udfVec{
    data *pt;
    unsigned int len;
} Vec;
```

### 三、结果与分析

>mpiicc main.c -lm -O -o 2.out && mpirun -n 4 ./2.out
>
>A.len = 60
>
> 5707    6059      46    3432    4091    -580   -1906    3114    2149    -689    3248   -1698   -1779    -357    -389     214    -645    4774    4515    3793    3395    -566   -1048    1519   -614     3928    2867   -1374     848    5836    5395     412    5751    -703    5892    3698     766    6034     668    4963    -798    5964    5313    -529    -537   -1219    1733     866   5603      105   -1484    2854    1587    -484   -1771    3022    5492    3144    3696     197
>
>-1906   -1779   -1771   -1698   -1484   -1374   -1219   -1048    -798    -703    -689    -645    -614    -580    -566    -537    -529    -484    -389    -357      46     105     197     214    412      668     766     848     866    1519    1587    1733    2149    2854    2867    3022    3114    3144    3248    3395    3432    3696    3698    3793    3928    4091    4515    4774   4963     5313    5395    5492    5603    5707    5751    5836    5892    5964    6034    6059

第一块为原始数据，第二块为排序后的数据。

### 四、备注：

​		这是在上次OpenMP实验中发现的问题：尾号线程分配到的数据一直是最多的，$0$号分配的一直是最少的。简单的分析可以发现，当我们正则采样时，我们取的样本是每个间隔的第一个数，是每个间隔中最小的，这样使得我们的主元相对于数据的$N$-分位数偏小，因此数据有向后分配的趋势，最后都转移给了最后一个线程。

​		在本次实验中，通过输出每个线程全局交换后得到的数据长度，仍然可以发现这个问题(之所以不直接输出中间数据，是因为这个输出不是原子的，直接输出可能会使不同线程的输出产生混淆)。负载不均衡的问题可以通过调整正则采样时选取的是每个间隔中的第几个数来解决，也可以通过改变选取主元位置的方式来解决。

### 五、总结：

​		本次实验主要用到了MPI中的聚合通信`MPI_Gather`、`MPI_Scatter`等函数的用法。不同于共享内存的OpenMP编程，本次实验让我认识到了分布式体系中通信的重要性。

___

源代码见附件。