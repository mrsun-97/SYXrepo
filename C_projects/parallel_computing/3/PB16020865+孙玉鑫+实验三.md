<h2 style="text-align:center">《并行计算》上机报告</h2>

<table>
	<tr>
		<td>姓名：</td>
		<td>孙玉鑫</td>
		<td>学号：</td>
		<td>PB16020865</td>
		<td>日期：</td>
		<td>2019/6/1</td>
	<tr/>
	<tr>
		<td>上机题目：</td>
		<td colspan="5" style="text-align:center">GPU实验</td>
	</tr>
	<tr>
		<td rowspan="2">实验环境</td> 
		<td>CPU</td>
		<td>内存</td>
         <td>GPU</td>
		<td>操作系统</td>
		<td>软件平台</td>
	</tr>
	<tr>
		<td>Intel i7-6700HQ ×8</td>
		<td>8G</td>
        <td>GeForce GTX 960M</td>
		<td>Arch Linux</td>
		<td>CUDA</td>
	</tr>
</table>


<br />

<h4 style="text-align:center">向量加法</h4>


### 一、算法设计与分析

#### 题目：

​		向量加法，定义A,B两个一维数组，编写GPU程序将A和B对应项相加，将结果保存在数组C中。分别测试数组规模为$10\mathrm{W}$、$20\mathrm{W}$、$100\mathrm{W}$、$200\mathrm{W}$、$1000\mathrm{W}$、$2000\mathrm{W} $时其与CPU加法的运行时间之比。

#### 算法设计：

​		利用GPU拥有大量线程的特性，将数组划分为等长的$\mathrm{N}$段，每段分配给一个block进行计算；在这$\mathrm{N}$段中每段有$\mathrm{n}$个数，将其分配给block中的$\mathrm{n}$个线程进行加和：

+ 生成数据：利用`rand()`函数生成两个等长的大数组。
+ 传输数据：利用`cudaMalloc()`函数在GPU的全局存储器中申请相同大小的空间，并利用`cudaMemcopy()`把数组传入GPU中。
+ GPU执行：GPU对传入的数据进行相加，并将结果存入全局存储器。
+ 传回数据：利用`cudaMemcopy()`将结果写入主机内存。

#### 算法分析：

​		利用GPU进行计算的优势是高度的并行化，但同时也有一定的缺点，就是需要进行数据移动。使用CPU进行计算，计算耗时在串行加法上；使用GPU进行计算，则需要数据传入、GPU计算、数据传出三个耗时的过程。

​		在数据分段时，我们不能保证每个线程块中的线程数整除数组长度，因此我们在计算需要的block数时要进行向上取整。

### 二、核心代码：

``` C
__global__ void add_vec(int *a, int *b, int *c){
    int k = blockIdx.x * blockDim.x + threadIdx.x;
    c[k] = a[k] + b[k];
}
```

​		自定义核函数完成相加：每个线程计算出自己负责的数据对应下标，并做一次加法运算。

### 三、结果与分析：

仅比较计算时间：

> nvcc main.cu -o add.out -g -O2
>
> size: 1.0E+05,  gpu: 0.012960,  cpu: 0.088960,  rate: 6.864198
>
> size: 2.0E+05,  gpu: 0.022560,  cpu: 0.294656,  rate: 13.060993
>
> size: 1.0E+06,  gpu: 0.175616,  cpu: 1.414112,  rate: 8.052296
>
> size: 2.0E+06,  gpu: 0.345760,  cpu: 2.793216,  rate: 8.078483
>
> size: 1.0E+07,  gpu: 1.707328,  cpu: 21.318497,  rate: 12.486468
>
> size: 2.0E+07,  gpu: 3.401344,  cpu: 27.669825,  rate: 8.134968



比较总时间：

> nvcc main.cu -o add.out -g -O2
>
> size: 1.0E+05,  gpu: 0.291808,  cpu: 0.100864,  rate: 0.345652
>
> size: 2.0E+05,  gpu: 0.447072,  cpu: 0.326432,  rate: 0.730155
>
> size: 1.0E+06,  gpu: 2.022880,  cpu: 1.562400,  rate: 0.772364
>
> size: 2.0E+06,  gpu: 7.400320,  cpu: 3.505760,  rate: 0.473731
>
> size: 1.0E+07,  gpu: 19.298689,  cpu: 16.910208,  rate: 0.876236
>
> size: 2.0E+07,  gpu: 37.494015,  cpu: 28.963392,  rate: 0.772480

​		在仅比较计算时间时，使用GPU获得了较高的加速比，但是在计入GPU的数据传输时间后，加速比反而小于$1$了。这说明在GPU计算时，数据在设备间的通信速度是加速比的瓶颈。实际上本算法的每个数据被送入GPU后，仅进行了一次加法就被送回，因此对数据的反复利用率不够高，因此会出现速度不如CPU的情况。

----



<h4 style="text-align:center">矩阵乘法</h4>

### 一、算法设计与分析

#### 题目： 

​		定义A，B 两个二维数组。使用GPU实现矩阵乘法。并对比串行程序，给出加速比。

#### 算法设计：

​		矩阵乘法具有以下性质：
$$
\bold{A} = \left(
\begin{matrix}
	a_{11} & a_{12} & \cdots & a_{1\mathrm{n}}\\
	a_{21} & a_{22} & \cdots & a_{2\mathrm{n}}\\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{\mathrm{mn}}
\end{matrix}
\right)
= \left(
\begin{matrix}
	\bold{A_{11}} & \cdots & \bold{A_{1\mathrm{s}}}\\
	\vdots & \ddots & \vdots \\
	\bold{A_{r1}} & \cdots & \bold{A_{\mathrm{rs}}}
\end{matrix}
\right)
$$
称为矩阵的分块，记做$\bold{A} = \left(\bold{A}_{ij}\right)_{r\times s}$（本题中子矩阵均为相同大小的方阵）。

若有$\bold{A}=\left(\bold{A}_{ij}\right)_{r\times s},\,\bold{B}=\left(\bold{B}_{ij}\right)_{s\times t},$ 则$\bold{AB}=\left(\bold{C}_{ij}\right)_{r\times t}$，其中$\bold{C}_{ij}=\sum_{k=1}^s\bold{A}_{ik}\bold{B}_{kj}$。

基于上述性质，我们可以利用共享内存的GPU矩阵乘算法完成本次试验：

+ 任务划分：将矩阵进行棋盘划分，得到多个子方阵。同时定义GPU二维网格，使网格中每个块对应一个方阵，定义块中的二维线程排布，使每个线程对应子方阵中的一个元素。
+ 数据传输：利用`cudaMemcopy()`将A，B两个矩阵传入GPU的全局内存中。
+ GPU计算：编号为$(i,j)$的线程块中所有线程并发读$\bold{A}_{ik}$与$\bold{B}_{kj}$中的数据到共享内存，做乘法$\bold{A}_{ik}\bold{B}_{kj}$，并对$k$循环，累加每次的结果，写入$\bold{C}_{ij}$中。上述子矩阵的乘法，由每个线程负责自己对应位置的元素，直接从共享内存中读取一行一列并做内积。
+ 数据传回：利用`cudaMemcopy()`，将$\bold{C}$矩阵传回主机内存。

#### 算法分析：

​    	该算法利用共享内存，节省了读写时间。原先计算每个元素时都需要单独从全局内存中读取所需矩阵元并计算，现在位于每个子方阵中的元素计算只需要一次全局内存读取，其余都从共享内存中读取，大大减少了访问全局内存的次数，提高计算访存比，使得计算速度有所提高。



### 二、核心代码：

以下代码来自徐云老师的ppt，描述了GPU中具体的计算过程。在计算$\bold{C}_{ij}$时，每个矩阵元的中间变量存储在对应线程的本地变量`Csub`中，累加完成后写入全局内存。

```c
//Kernel code
__global__ void matrixMul( float* C, float* A, float* B, int wA, int wB)
{
    // Declaration of the shared memory array As used to
    // store the sub-matrix of A
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];

    // Declaration of the shared memory array Bs used to
    // store the sub-matrix of B
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

	// Block index
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // Thread index
    int tx = threadIdx.x;
    int ty = threadIdx.y;     

    // Csub is used to store the element of the block sub-matrix
    // that is computed by the thread
    float Csub = 0;

    // Loop over all the sub-matrices of A and B
    // required to compute the block sub-matrix
    for (int m= 0; m<wA/BLOCK_SIZE; m++) 
	{
		//get the address of submatrixA
		//float *subA=A+wA*BLOCK_SIZE*by+BLOCK_SIZE*m;
		float *subA=GetSubMatrix(A, m, by, wA);
		//get the address of submatrixB
		//float *subB=B+wB*BLOCK_SIZE*m+BLOCK_SIZE*bx;
		float *subB=GetSubMatrix(B, bx, m, wB);
        // Load the matrices from device memory
        // to shared memory; each thread loads
        // one element of each matrix
        As[ty][tx] = *(subA+ wA * ty + tx);
        Bs[ty][tx] = *(subB+ wB * ty + tx);

        // Synchronize to make sure the matrices are loaded
        __syncthreads();

        // Multiply the two matrices together;
        // each thread computes one element
        // of the block sub-matrix
        for (int k = 0; k < BLOCK_SIZE; ++k)
            Csub += As[ty][k] * Bs[k][tx];

        // Synchronize to make sure that the preceding
        // computation is done before loading two new
        // sub-matrices of A and B in the next iteration
        __syncthreads();
    }

    // Write the block sub-matrix to device memory;
    // each thread writes one element
    //float *subC = C+wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
	float *subC=GetSubMatrix(C, bx, by, wB);
    *(subC + wB * ty + tx)= Csub;
}
```



### 三、结果与分析

计入总时间：

> nvcc mp.cu -o mp.out -g -O2
>
> GPU processing time: 113.051361 (ms) 
>
> please wait...
>
> CPU processing time: 88574.359375 (ms) 
>
> accelerate rate: 783.487793

仅计入计算时间：

>nvcc mp.cu -o mp.out -g -O2
>
>GPU processing time: 97.201920 (ms) 
>
>please wait...
>
>CPU processing time: 85257.914062 (ms) 
>
>accelerate rate: 877.121704

加速比始终很大，说明相比于串行计算，GPU计算大幅提高了计算速度。

​		对比上文的向量加法实验，本实验计算时间约$97\mathrm{ms}$，而传输时间约为$113-97=16\,\mathrm{ms}$，传输时间占比要小很多。这是因为主机到设备的传输数据量是$O(n^2)$的，但计算所需工作量是$O(n^3)$的，问题本身的计算访存比高，并且进行了矩阵分块，利用共享内存进行优化，因此加速比较高。

### 四、备注：

​		以上矩阵乘法默认矩阵宽度可以被分块宽度整除，若不能整除，可以在`cudaMemcopy()`的同时将矩阵添加$0$元素变为可以整除的行列数，之后再进行计算。计算完成后，每行`cudaMemopy()`仅将原先存在的矩阵元送回即可。

### 五、总结：

​		在这次实验实验中，我了解了GPU编程的基本过程，认识到了GPU的工作原理，收获颇丰。同时，我也认识到了一些事实，比如在向量加法实验中，任务的计算访存比很低，因此不论任务量有多少，加速比总不会太高(甚至没有起到加速作用)；在考虑GPU加速效果时，不能忽略掉数据在设备间传输时，与计算时间有同等量级的传输时间，即要尽量在GPU上使用计算访存比大的算法。

___

源代码见附件。